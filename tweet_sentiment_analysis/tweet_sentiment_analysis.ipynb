{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf37f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "df = pd.read_csv(r\"C:\\Users\\Zribi Ahmed\\Desktop\\projectML\\train.csv\")\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d35231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0                                          I`d have responded, if I were going   \n",
       "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afad5be4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Each Column:\n",
      "textID           0\n",
      "text             1\n",
      "selected_text    1\n",
      "sentiment        0\n",
      "dtype: int64\n",
      "\n",
      "Total Missing Values in the DataFrame:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "missing_values = df.isna() \n",
    "\n",
    "missing_count = missing_values.sum()\n",
    "\n",
    "total_missing_count = missing_count.sum()\n",
    "\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(missing_count)\n",
    "\n",
    "print(\"\\nTotal Missing Values in the DataFrame:\")\n",
    "print(total_missing_count)\n",
    "\n",
    "df['text'].fillna('', inplace=True)\n",
    "df['selected_text'].fillna('', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f387026d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fa6f8_row0_col1 {\n",
       "  background-color: #3f007d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_fa6f8_row1_col1 {\n",
       "  background-color: #dcdcec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fa6f8_row2_col1 {\n",
       "  background-color: #fcfbfd;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fa6f8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fa6f8_level0_col0\" class=\"col_heading level0 col0\" >sentiment</th>\n",
       "      <th id=\"T_fa6f8_level0_col1\" class=\"col_heading level0 col1\" >text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fa6f8_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_fa6f8_row0_col0\" class=\"data row0 col0\" >neutral</td>\n",
       "      <td id=\"T_fa6f8_row0_col1\" class=\"data row0 col1\" >11118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa6f8_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_fa6f8_row1_col0\" class=\"data row1 col0\" >positive</td>\n",
       "      <td id=\"T_fa6f8_row1_col1\" class=\"data row1 col1\" >8582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa6f8_level0_row2\" class=\"row_heading level0 row2\" >0</th>\n",
       "      <td id=\"T_fa6f8_row2_col0\" class=\"data row2 col0\" >negative</td>\n",
       "      <td id=\"T_fa6f8_row2_col1\" class=\"data row2 col1\" >7781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ef510025b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff88cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='sentiment',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e647db12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27481, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e71c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def clean_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lowercasing\n",
    "    lowercased_text = text.lower()\n",
    "\n",
    "    # Removing punctuation\n",
    "    text_without_punctuation = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return lemmatized_text  # You can return any cleaned version you prefer\n",
    "\n",
    "# Apply the cleaning function to the 'text_column'\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Display the DataFrame with cleaned text\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e217386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4506c292376427b9baadaa2f51f92c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to classify sentiment\n",
    "def classify_sentiment(text):\n",
    "    # Initialize sentiment scores\n",
    "    compound_score = 0\n",
    "\n",
    "    # Check for specific words and classify as 'negative' if found\n",
    "    negative_words = ['*']  # Add your list of negative words\n",
    "    for word in negative_words:\n",
    "        if word in text:\n",
    "            return 'negative'\n",
    "\n",
    "    # Calculate sentiment scores for each word and aggregate them\n",
    "    for word in text:\n",
    "        sentiment = sia.polarity_scores(word)\n",
    "        compound_score += sentiment['compound']\n",
    "\n",
    "    # Determine sentiment class based on the aggregate compound score\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply the sentiment analysis function to the 'cleaned_data' column\n",
    "tqdm.pandas()  # Enable progress bar with tqdm\n",
    "df['sent'] = df['cleaned_text'].progress_apply(classify_sentiment)\n",
    "\n",
    "# Display the DataFrame with sentiment classification\n",
    "#print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c0feb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[I, `, d, have, responded, ,, if, I, were, going]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Sooo, SAD, I, will, miss, you, here, in, San, Diego, !, !, !]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>[my, bos, is, bullying, me, ...]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>[what, interview, !, leave, me, alone]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, release, we, already, bought]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0                                          I`d have responded, if I were going   \n",
       "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                         selected_text sentiment  \\\n",
       "0  I`d have responded, if I were going   neutral   \n",
       "1                             Sooo SAD  negative   \n",
       "2                          bullying me  negative   \n",
       "3                       leave me alone  negative   \n",
       "4                        Sons of ****,  negative   \n",
       "\n",
       "                                                                                           cleaned_text  \\\n",
       "0                                                     [I, `, d, have, responded, ,, if, I, were, going]   \n",
       "1                                        [Sooo, SAD, I, will, miss, you, here, in, San, Diego, !, !, !]   \n",
       "2                                                                      [my, bos, is, bullying, me, ...]   \n",
       "3                                                                [what, interview, !, leave, me, alone]   \n",
       "4  [Sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, release, we, already, bought]   \n",
       "\n",
       "       sent  \n",
       "0   neutral  \n",
       "1  negative  \n",
       "2  negative  \n",
       "3  negative  \n",
       "4  negative  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbce5d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using VADER Seniment Scoring: 0.6187547760270733\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame df with 'sentiment' and 'sent' columns\n",
    "\n",
    "# Count the matching rows using pandas\n",
    "matching_rows = df[df['sentiment'] == df['sent']]\n",
    "count = len(matching_rows)\n",
    "\n",
    "print(\"Accuracy using VADER Seniment Scoring:\",count/df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45ae1a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy rows with sentiment 'neutral': 0.44702284583558194\n",
      "accuracy rows with sentiment 'positive': 0.8465392682358425\n",
      "accuracy rows with sentiment 'negative': 0.6129032258064516\n"
     ]
    }
   ],
   "source": [
    "matching_neutral_rows = df[(df['sentiment'] == 'neutral') & (df['sent'] == 'neutral')]\n",
    "count_matching_neutral = len(matching_neutral_rows)\n",
    "neutral = df[df['sentiment'] == 'neutral']\n",
    "n_neutral=len(neutral)\n",
    "print(\"accuracy rows with sentiment 'neutral':\", count_matching_neutral/n_neutral)\n",
    "matching_positive_rows = df[(df['sentiment'] == 'positive') & (df['sent'] == 'positive')]\n",
    "count_matching_positive = len(matching_positive_rows)\n",
    "positive = df[df['sentiment'] == 'positive']\n",
    "n_positive=len(positive)\n",
    "print(\"accuracy rows with sentiment 'positive':\", count_matching_positive/n_positive)\n",
    "matching_negative_rows = df[(df['sentiment'] == 'negative') & (df['sent'] == 'negative')]\n",
    "count_matching_negative = len(matching_negative_rows)\n",
    "negative = df[df['sentiment'] == 'negative']\n",
    "n_negative=len(negative)\n",
    "print(\"accuracy rows with sentiment 'negative':\", count_matching_negative/n_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7498acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46446b33a65447309a01f10d222d5eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to classify sentiment based on the highest sentiment score\n",
    "def classify_sentiment(word_list):\n",
    "    text = ' '.join(word_list)  # Convert the list of words to a single string\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "\n",
    "    # Check for specific words and classify as 'negative' if found\n",
    "    negative_words = ['*']  # Add your list of negative words\n",
    "    for word in negative_words:\n",
    "        if word in text:\n",
    "            return 'negative'\n",
    "\n",
    "    # Determine the sentiment class based on the compound score\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply the sentiment analysis function to the 'cleaned_data' column\n",
    "tqdm.pandas()  # Enable progress bar with tqdm\n",
    "df['sent'] = df['cleaned_text'].progress_apply(classify_sentiment)\n",
    "\n",
    "# Display the DataFrame with sentiment classification\n",
    "#print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f29b1385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[I, `, d, have, responded, ,, if, I, were, going]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Sooo, SAD, I, will, miss, you, here, in, San, Diego, !, !, !]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>[my, bos, is, bullying, me, ...]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>[what, interview, !, leave, me, alone]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, release, we, already, bought]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0                                          I`d have responded, if I were going   \n",
       "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                         selected_text sentiment  \\\n",
       "0  I`d have responded, if I were going   neutral   \n",
       "1                             Sooo SAD  negative   \n",
       "2                          bullying me  negative   \n",
       "3                       leave me alone  negative   \n",
       "4                        Sons of ****,  negative   \n",
       "\n",
       "                                                                                           cleaned_text  \\\n",
       "0                                                     [I, `, d, have, responded, ,, if, I, were, going]   \n",
       "1                                        [Sooo, SAD, I, will, miss, you, here, in, San, Diego, !, !, !]   \n",
       "2                                                                      [my, bos, is, bullying, me, ...]   \n",
       "3                                                                [what, interview, !, leave, me, alone]   \n",
       "4  [Sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, release, we, already, bought]   \n",
       "\n",
       "       sent  \n",
       "0   neutral  \n",
       "1  negative  \n",
       "2  negative  \n",
       "3  negative  \n",
       "4  negative  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc5a12d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using updated VADER Seniment Scoring: 0.62661475201048\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame df with 'sentiment' and 'sent' columns\n",
    "\n",
    "# Count the matching rows using pandas\n",
    "matching_rows = df[df['sentiment'] == df['sent']]\n",
    "count = len(matching_rows)\n",
    "\n",
    "print(\"Accuracy using updated VADER Seniment Scoring:\",count/df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "922df361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy rows with sentiment 'neutral': 0.43883792048929665\n",
      "accuracy rows with sentiment 'positive': 0.8506175716616173\n",
      "accuracy rows with sentiment 'negative': 0.6478601722143683\n"
     ]
    }
   ],
   "source": [
    "matching_neutral_rows = df[(df['sentiment'] == 'neutral') & (df['sent'] == 'neutral')]\n",
    "count_matching_neutral = len(matching_neutral_rows)\n",
    "neutral = df[df['sentiment'] == 'neutral']\n",
    "n_neutral=len(neutral)\n",
    "print(\"accuracy rows with sentiment 'neutral':\", count_matching_neutral/n_neutral)\n",
    "matching_positive_rows = df[(df['sentiment'] == 'positive') & (df['sent'] == 'positive')]\n",
    "count_matching_positive = len(matching_positive_rows)\n",
    "positive = df[df['sentiment'] == 'positive']\n",
    "n_positive=len(positive)\n",
    "print(\"accuracy rows with sentiment 'positive':\", count_matching_positive/n_positive)\n",
    "matching_negative_rows = df[(df['sentiment'] == 'negative') & (df['sent'] == 'negative')]\n",
    "count_matching_negative = len(matching_negative_rows)\n",
    "negative = df[df['sentiment'] == 'negative']\n",
    "n_negative=len(negative)\n",
    "print(\"accuracy rows with sentiment 'negative':\", count_matching_negative/n_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd8a624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentiments\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Apply the sentiment analysis function to the 'cleaned_text' column\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Map the sentiment indices to the categories\u001b[39;00m\n\u001b[0;32m     33\u001b[0m sentiment_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36mclassify_sentiment\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     22\u001b[0m probs \u001b[38;5;241m=\u001b[39m softmax(logits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Determine the sentiment class based on the highest probability\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39margmax(probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m probs \u001b[38;5;129;01min\u001b[39;00m probs]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentiments\n",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m probs \u001b[38;5;241m=\u001b[39m softmax(logits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Determine the sentiment class based on the highest probability\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m probs \u001b[38;5;129;01min\u001b[39;00m probs]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentiments\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\ML\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1195\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argmax_dispatcher)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;124;03m    Returns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \n\u001b[0;32m   1194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ML\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define the DistilBERT-based model\n",
    "MODEL = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Load the DistilBERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Function to classify sentiment using DistilBERT\n",
    "def classify_sentiment(texts):\n",
    "    text_strings = [' '.join(text) for text in texts]  # Convert the list of words to a list of single strings\n",
    "    inputs = tokenizer(text_strings, padding=True, truncation=True, max_length=128, return_tensors='pt', is_split_into_words=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = softmax(logits.detach().numpy(), axis=1)\n",
    "    \n",
    "    # Determine the sentiment class based on the highest probability\n",
    "    sentiments = [np.argmax(probs, axis=1) for probs in probs]\n",
    "\n",
    "    return sentiments\n",
    "\n",
    "# Apply the sentiment analysis function to the 'cleaned_text' column\n",
    "sentiments = classify_sentiment(df['cleaned_text'])\n",
    "\n",
    "# Map the sentiment indices to the categories\n",
    "sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "sentiments = [sentiment_map[sentiment[0]] for sentiment in sentiments]\n",
    "\n",
    "# Ensure that the number of sentiments matches the number of rows in df\n",
    "if len(sentiments) != len(df):\n",
    "    print(f\"Warning: Length of sentiments ({len(sentiments)}) does not match length of the DataFrame ({len(df)}).\")\n",
    "\n",
    "# Assign sentiments to the DataFrame\n",
    "df['sent'] = sentiments\n",
    "\n",
    "# Display the DataFrame with sentiment classification\n",
    "#print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d55d807f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[I, `, d, have, responded, ,, if, I, were, going]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Sooo, SAD, I, will, miss, you, here, in, San, Diego, !, !, !]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>[my, bos, is, bullying, me, ...]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>[what, interview, !, leave, me, alone]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>[Sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, release, we, already, bought]</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  \\\n",
       "0  cb774db0d1   \n",
       "1  549e992a42   \n",
       "2  088c60f138   \n",
       "3  9642c003ef   \n",
       "4  358bd9e861   \n",
       "\n",
       "                                                                          text  \\\n",
       "0                                          I`d have responded, if I were going   \n",
       "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
       "2                                                    my boss is bullying me...   \n",
       "3                                               what interview! leave me alone   \n",
       "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
       "\n",
       "                         selected_text sentiment  \\\n",
       "0  I`d have responded, if I were going   neutral   \n",
       "1                             Sooo SAD  negative   \n",
       "2                          bullying me  negative   \n",
       "3                       leave me alone  negative   \n",
       "4                        Sons of ****,  negative   \n",
       "\n",
       "                                                                                           cleaned_text  \\\n",
       "0                                                     [I, `, d, have, responded, ,, if, I, were, going]   \n",
       "1                                        [Sooo, SAD, I, will, miss, you, here, in, San, Diego, !, !, !]   \n",
       "2                                                                      [my, bos, is, bullying, me, ...]   \n",
       "3                                                                [what, interview, !, leave, me, alone]   \n",
       "4  [Sons, of, *, *, *, *, ,, why, couldn, `, t, they, put, them, on, the, release, we, already, bought]   \n",
       "\n",
       "       sent  \n",
       "0   neutral  \n",
       "1  negative  \n",
       "2  negative  \n",
       "3  negative  \n",
       "4   neutral  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3cb080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
